1. System Requirements:

System: ubuntu version: 18.04
Software: python version: 3.7.11
We run these codes using Nvidia Titan Xp
We attached a small dataset in dataset/regression/sample.

2. Installation guide
One can setup the environment using anaconda, the typical install time is less than an hour, but it depends on the internet speed.
the command is:
conda env create -f environment.yml
then activate the environment through command:
conda activate tfq

3. Demo
One may run our code using the script train.py inside the folder main.
An example command is: 
python train.py regression sample 0 10 6 6 32 32 1e-4 0.99 10 2000 test_log
corresponds to regression on dataset "sample", 
with 0 modification, 
10 bond dimension, 
the hyperparameter for encoding MPS and MPO are 6,
the batch size is 32 for test and train
the learning rate is 1e-4
the decay interval is 10
the number of epoches is 2000
the output is saved in test_log.txt in the folder output.

Notice that the output entirely depends on the TN contraction results, which comes from the matrix production. If the TN contraction results for each parts are known, they can be directly added without abundant calculations.
Like the molecule C formed by A and B 
TN_contraction results  of C can be directly obtained by the adding of TN_contraction results of A and TN_contraction results of B.

The expected output is:
"""
Epoch	Time(sec)	Loss_train	MAE_dev	MAE_test
1	24.78169422596693	51343368.59460449	57.13965818951954	59.022087139204984
2	48.64706811727956	5353620.6083984375	56.775621237657944	58.648000369387645
3	73.6783681162633	3326552.158023834	5.015739998483766	4.979550294293239
4	99.06186244916171	47647.09996366501	3.3494113207401592	3.2787066688676645
5	124.90278318338096	17699.269937992096	2.517828446209565	2.454029134666454
6	151.05004501715302	12096.532528162003	2.465032883207093	2.3985452272836016
7	176.67972720600665	9522.050415158272	1.651043267605267	1.6169817954329624
8	202.02122848993167	8056.3915021419525	1.707843640587938	1.673878884370058
9	227.56449614511803	7510.977852940559	2.220422622043446	2.2247274071723413
10	252.79444259498268	6773.867526173592	2.9346418660596587	2.892229346488506
11	278.38273243419826	5844.610919833183	1.7966580487804542	1.7408228666689212
12	304.6248670099303	5336.440129995346	1.3312141104153532	1.2990195098201773
13	330.0119587611407	4455.095820188522	1.1720185462831105	1.1518729470933586
14	355.9923691479489	4483.845223963261	1.1698516712231777	1.1505056550828765
15	381.89734259806573	3755.097065091133	0.9912931364761249	0.9648457645430164
16	408.04735829494894	3558.078502237797	1.0022028907965321	0.9856898039490972
17	433.52822756720707	2783.797519683838	2.1459914136417293	2.1208437060849024
18	459.1736560901627	2830.7541399002075	0.9078125189443205	0.9027856256210613
19	484.39730715192854	2497.115588068962	1.0574327796092151	1.0424868370341358
20	510.3121106121689	2146.0036296844482	2.012883636267944	2.017228299443516
21	535.6606463082135	1937.9427976310253	1.0071707137820414	0.9779918625707029
22	561.5225708163343	1768.6581134796143	0.8394700991111738	0.8214226722959806
23	587.372891546227	1682.8617333471775	1.0779298907208927	1.0729514129527435
24	612.9744792389683	1549.337517902255	0.8581269567760872	0.8348984284613954
25	638.8437423920259	1376.268357872963	0.6493758014308679	0.6365043974492655
26	664.6546911471523	1266.518048092723	0.5558459968653006	0.5326639472746066
27	689.5049184020609	1201.810548067093	1.1037408126934387	1.0758571387085054
28	715.5855665993877	1086.1501273065805	1.037957374452199	1.0152387542220165
29	741.6731850053184	1054.8554305583239	0.6638815623642897	0.6576861421708445
30	767.3523864499293	1089.8356424421072	0.5543228455106507	0.5370267729158168
31	792.9044083072804	970.2271043360233	1.4426734808067168	1.4295252157872416
32	818.3116256021895	944.7649857103825	0.4045856833188851	0.38620793561748684
33	844.1889078933746	963.8243656903505	0.4623369363306877	0.4407117694396533
34	869.7446858733892	927.0319001376629	0.5058123198911784	0.49117719471287885
35	895.5042123012245	863.507352694869	0.5774259072125093	0.5493541535476694
36	920.8117446759716	820.9113912433386	0.9743523457787645	0.9533624560931805
37	946.4548422470689	815.5772014409304	1.8891347407218295	1.8629450230939193
38	971.5289327711798	806.8583689033985	0.3958952539930494	0.3849497109012421
39	997.2946869679727	754.492011114955	0.6372584995243942	0.6220491116174431
40	1023.0332475122996	684.4149344190955	0.5761905954330824	0.5588093267979032
41	1048.3719835011289	755.5174430608749	1.115392717227979	1.0998279551808143
42	1074.4862732272595	813.068672157824	0.43121577493224134	0.4100832575770306
43	1100.3709222292528	653.5211965590715	0.6078372270743411	0.5887384093736424
44	1125.2788970889524	812.8339619934559	0.6007957824465922	0.586728899804497
45	1150.406789997127	654.2358357533813	1.395423869785283	1.39109199694014
46	1175.566672415007	664.4801099225879	0.35596285662855576	0.3401591925133161
47	1200.8941955873743	691.7574156373739	0.47610542736527073	0.46319491541708
48	1226.9566766959615	725.9581271037459	1.157087864240728	1.138035137127686
49	1251.9965591570362	581.574426099658	0.5230952837666324	0.5000088342400014
50	1277.0304239271209	650.6495333239436	0.44613803751581677	0.4337942031740061
51	1302.8013879433274	616.207222469151	0.7255843788990856	0.7130212889714413
52	1328.4564306433313	602.4222933873534	0.7385066013034794	0.7246812330703367
53	1354.3444196050987	598.8245338201523	0.8578288076155342	0.8370286327626247
54	1379.1035980582237	596.5611332505941	0.32355020708208965	0.30595281797717705
55	1404.2000521901064	633.7795120477676	0.6177342546174542	0.6077254029303332
56	1429.7094226432964	562.1216160468757	1.6280127669564757	1.6050962212227802
57	1454.453058296349	521.2200335636735	1.4908097699856382	1.4703562885985149
58	1479.9773228731938	622.3259990401566	0.622766765999202	0.6060330988421646
59	1505.3770893043838	505.50242076441646	0.9567050288008783	0.9348210037851006
60	1531.6828250549734	533.5025856979191	0.5495285481954536	0.5310178522114998
61	1556.4811045341194	546.0577139258385	0.9213234318029262	0.8897556241229397
62	1581.943211730104	531.3588733114302	0.5857171818580369	0.5777000303721948
63	1607.7510546711273	547.0922458320856	0.28878857373653094	0.27621273575288274
64	1633.782092554029	498.8806484416127	0.31099647539194764	0.30441295432672566
65	1659.4464130261913	476.6989441998303	0.2983700986909543	0.2838549360517951
66	1685.6584334783256	517.8283757865429	0.5619815413085386	0.5393688489372794
67	1711.5373671599664	468.11048217490315	0.32987145107432897	0.31558025890743874
68	1737.387212665286	468.4182649999857	0.47075475903719866	0.4423176978133013
69	1763.3891968559474	511.12743206322193	0.4012105674829763	0.38975404417796733
70	1789.2828967422247	465.88190706819296	0.29241233162632374	0.27482095685885943
71	1814.3107278989628	440.4620707333088	0.32660926061074835	0.3187510028091176
72	1839.5273672882468	499.29603131860495	0.2977902969980348	0.2816696917191192
73	1864.5641532270238	455.3045376613736	0.3382983175411181	0.3168380671338913
74	1889.8135779420845	499.77682035416365	0.46203792175912967	0.45225063917989944
75	1915.6514058280736	492.6629808358848	0.3267209739771169	0.30854037398755213
76	1941.438307538163	436.4349234737456	0.9204667696296227	0.9063635639612014
77	1966.5980086391792	425.3699425123632	1.0331768440608247	1.0230918600365824
78	1992.4564747689292	393.56505220010877	0.39822502868170123	0.37944152604384657
79	2017.5230046240613	452.1186960712075	0.5023758072347189	0.4991685124062841
80	2043.2149082799442	441.28274525702	0.6470382143627709	0.6229905247071885
81	2067.970129502937	429.96423019096255	0.32737464560342966	0.32114824755995963
82	2093.383451558184	360.94049044139683	0.2829915341620521	0.2741691742628886
83	2119.1085924073122	419.7649939954281	0.4301432422267664	0.4045533253559068
84	2144.324086262379	441.37103017792106	0.8898282944483359	0.8840245105173968
85	2169.2718982859515	402.4773134365678	0.302998609521319	0.2826326988991756
86	2194.4412329862826	458.2565684635192	0.2666921206575484	0.2516655258640471
87	2219.7663954123855	390.9056817665696	0.4928591967167219	0.4902628715678758
88	2245.1584199643694	388.4535642005503	0.2525007148897675	0.23774792964735142
89	2270.4843817739747	418.8152436055243	0.34592264877216006	0.3291335778892409
90	2295.6630705902353	427.15297055244446	1.2703514594256744	1.254996613161922
91	2320.5943428752944	361.611716337502	0.32274304340440046	0.3051832774195356
92	2345.84183371393	367.4492797702551	0.6447275669795396	0.6362814260173816
93	2371.0904114730656	376.3568788021803	0.28080396027920207	0.26671049074873215
94	2396.4722778243013	358.2592386044562	0.4828707972713841	0.46214047126611574
95	2422.179460008163	366.1323618963361	0.3975264547102607	0.37725744387290433
96	2448.134132136125	427.8450008034706	0.8288659425137005	0.8193712350098856
97	2473.464181341231	318.3500133752823	0.5890991865377663	0.568287185807344
98	2499.0725352191366	428.0589308552444	0.23109583284192914	0.21923439767670508
99	2525.1652024141513	355.93498346582055	0.6645401831943348	0.665043376144276
100	2550.6351380981505	336.35873815976083	0.8899836615568897	0.8875772758005959
"""
The expected run time is listed on the output.

4. Instructions for use
If you want to run AFA on your data:
    1. place the data in folders like dataset/regression/YOURSAMPLEDATA, rename them as data_test.txt and data_train.txt
    2. run thecodes using python train.py regression YOURSAMPLEDATA 0 10 6 6 32 32 1e-4 0.99 10 2000 test_log 
        Here "YOURSAMPLEDATA" shall be replaced by the name of your dataset.
