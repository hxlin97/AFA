{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is a sample implementation for AFA. We demonstrate that AFA can obtain the properties through the adding of its fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocess as pp\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.First we obtain an AFA model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the AFA model\n",
    "class AFA(nn.Module):\n",
    "    def __init__(self, N_fingerprints, N_bond_fingerprints, dim, layer_hidden, layer_output):\n",
    "        super(AFA, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(N_fingerprints, dim)\n",
    "        self.embed_bond_fingerprint = nn.Embedding(N_bond_fingerprints, dim)\n",
    "        # you can revise this layer for more information\n",
    "        # this dim corresponds to the TN states of each atom\n",
    "        self.W_fingerprint = nn.Linear(dim, dim)\n",
    "        self.W_bond_fingerprint = nn.Linear(dim, dim)\n",
    "        self.W_output = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                       for _ in range(layer_output)])\n",
    "        self.W_property = nn.Linear(dim, 1)\n",
    "\n",
    "    def pad(self, matrices, pad_value, device=\"cuda:0\"):\n",
    "        \"\"\"Only for batch process\"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        zeros = torch.FloatTensor(np.zeros((M, N))).to(device)\n",
    "        pad_matrices = pad_value + zeros\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "\n",
    "    def pad_MPO(self, matrices, pad_value, device=\"cuda:0\"):\n",
    "        \"\"\"Only for batch process\"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        zeros = torch.LongTensor(np.zeros((M, N))).to(device)\n",
    "        pad_matrices = pad_value + zeros\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "\n",
    "    def to_output(self, x):\n",
    "        for l in range(len(self.W_output)):\n",
    "            x = self.W_output[l](x)\n",
    "            x = torch.relu(x)\n",
    "        outputs = self.W_property(x)\n",
    "        return outputs\n",
    "\n",
    "    def update_MPS(self, adjacencies, MPSs):\n",
    "        contri_MPS = torch.relu(self.W_fingerprint(MPSs))\n",
    "        # the contribution from nearby atoms\n",
    "        # you can repeat this step for n times, so that n-nearest atoms contributes to the atom\n",
    "        return MPSs + torch.matmul(adjacencies, contri_MPS)\n",
    "\n",
    "    def update_MPO(self, bond_adjacencies, MPOs):\n",
    "        contri_MPO = torch.relu(self.W_bond_fingerprint(MPOs))\n",
    "        # the contribution from nearby atoms\n",
    "        # you can repeat this step for n times, so that n-nearest atoms contributes to the atom\n",
    "        return MPOs + torch.matmul(bond_adjacencies, contri_MPO)\n",
    "\n",
    "    def tn_contraction(self, inputs):\n",
    "        \"\"\"Construct the MPS for each atom and then contract them\"\"\"\n",
    "        fingerprints, adjacencies, bond_fingerprints, bond_adjacencies, bond_index, molecular_sizes = inputs\n",
    "\n",
    "        fingerprints = torch.cat(fingerprints)\n",
    "        bond_fingerprints = torch.cat(bond_fingerprints)\n",
    "        adjacencies = self.pad(adjacencies, 0) # 0 means false\n",
    "        bond_index = self.pad_MPO(bond_index, 0)\n",
    "        MPSs = self.embed_fingerprint(fingerprints)\n",
    "        MPOs = self.embed_bond_fingerprint(bond_index)\n",
    "        MPSs = self.update_MPS(adjacencies, MPSs)\n",
    "        MPSs = F.normalize(MPSs, 2, 1)\n",
    "        MPOs = F.normalize(MPOs, 2, 1)\n",
    "        MPOs = torch.einsum(\"abc,ab->abc\", MPOs, adjacencies)\n",
    "        tmp = torch.einsum(\"ac,abc->cb\", MPSs, MPOs)\n",
    "        # tmp = torch.einsum(\"ab,ac->bc\", MPSs, adjacencies)\n",
    "        info_sets = torch.split(tmp, molecular_sizes,dim=1)\n",
    "        fv_sets = torch.split(MPSs, molecular_sizes)\n",
    "        TN_results = [torch.einsum(\"bc,cb->b\", i, j) for (i,j) in zip(info_sets, fv_sets)]\n",
    "        TN_results = torch.stack(TN_results)\n",
    "        return TN_results\n",
    "\n",
    "    def forward(self, data_batch, train):\n",
    "        inputs = data_batch[:-1]\n",
    "        correct_values = torch.cat(data_batch[-1])\n",
    "        if train:\n",
    "            molecular_vectors = self.tn_contraction(inputs)\n",
    "            predicted_values = self.to_output(molecular_vectors)\n",
    "            loss = F.mse_loss(predicted_values, correct_values)\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                molecular_vectors = self.tn_contraction(inputs)\n",
    "                predicted_values = self.to_output(molecular_vectors)\n",
    "            predicted_values = predicted_values.to('cpu').data.numpy()\n",
    "            correct_values = correct_values.to('cpu').data.numpy()\n",
    "            predicted_values = np.concatenate(predicted_values)\n",
    "            correct_values = np.concatenate(correct_values)\n",
    "            return predicted_values, correct_values\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch_train):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_train]))\n",
    "            loss = self.model.forward(data_batch, train=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "        return loss_total/batch_train\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset):\n",
    "        N = len(dataset)\n",
    "        SAE = 0  # sum absolute error.\n",
    "        for i in range(0, N, batch_test):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_test]))\n",
    "            predicted_values, correct_values = self.model.forward(\n",
    "                                               data_batch, train=False)\n",
    "            SAE += sum(np.abs(predicted_values-correct_values))\n",
    "        MAE = SAE / N  # mean absolute error.\n",
    "        return MAE/batch_test\n",
    "\n",
    "    def save_result(self, result, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(result + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Preprocessing\n",
      "data_train.txt\n",
      "Valence of atom 2 is 5 which bigger than allowed max 4 . Stopping\t-421.704612\n",
      "Valence of atom 2 is 5 which bigger than allowed max 4 . Stopping\t-457.674883\n",
      "Valence of atom 5 is 5 which bigger than allowed max 4 . Stopping\t-421.737229\n",
      "data_test.txt\n",
      "Valence of atom 4 is 5 which bigger than allowed max 4 . Stopping\t-382.406252\n",
      "Valence of atom 2 is 5 which bigger than allowed max 4 . Stopping\t-456.479858\n",
      "Valence of atom 3 is 5 which bigger than allowed max 4 . Stopping\t-456.513638\n",
      "Valence of atom 5 is 5 which bigger than allowed max 4 . Stopping\t-456.460476\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The preprocess has finished!\n",
      "# of training data samples: 31896\n",
      "# of development data samples: 3544\n",
      "# of test data samples: 11797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creating a model.\n",
      " the number of fingerprints are 71\n",
      "# of model parameters: 24401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Start training.\n",
      "The result is saved in the output directory every epoch!\n",
      "The training will finish in about 0 hours 9 minutes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch\tTime(sec)\tLoss_train\tMAE_dev\tMAE_test\n",
      "1\t58.53926040278748\t2150465.6648988724\t0.7573007136650602\t0.7351709423079736\n",
      "2\t116.44719763798639\t18981.28723335266\t0.39606058099738095\t0.39424271444915826\n",
      "3\t174.48055599979125\t9034.329448103905\t0.33079346859697295\t0.33315826078749433\n",
      "4\t232.72693814197555\t7246.162724494934\t0.35332753225050983\t0.3575410331175713\n",
      "5\t290.41150323697366\t6115.587152123451\t0.2689663689776952\t0.26686545700463865\n",
      "6\t348.2122652619146\t5175.199206233025\t0.26605728773177345\t0.26601008531389836\n",
      "7\t405.73082397389226\t4468.671099901199\t0.26459619867075107\t0.2662489667851874\n",
      "8\t463.8075293789152\t3790.2766588926315\t0.2636593868178116\t0.2680396205885608\n",
      "9\t521.3945918008685\t3260.7115648388863\t0.23328246637068806\t0.23619760391398326\n",
      "10\t579.0375823248178\t2793.9742367863655\t0.22864769651443104\t0.23269575996460365\n"
     ]
    }
   ],
   "source": [
    "(task, dataset, radius, dim, layer_hidden, layer_output, batch_train, \n",
    " batch_test, lr, lr_decay, decay_interval, iteration, setting) = ('regression', 'CHO', \n",
    "'1', '50', '6', '6', '32', '32', '1e-4', '0.99', '10', '10', 'sample')\n",
    "(radius, dim, layer_hidden, layer_output,\n",
    " batch_train, batch_test, decay_interval,\n",
    " iteration) = map(int, [radius, dim, layer_hidden, layer_output,\n",
    "                        batch_train, batch_test,\n",
    "                        decay_interval, iteration])\n",
    "lr, lr_decay = map(float, [lr, lr_decay])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Error: GPU required')\n",
    "print('-'*100)\n",
    "\n",
    "print('Preprocessing')\n",
    "(dataset_train, dataset_dev, dataset_test,\n",
    " N_fingerprints, N_bond_fingerprints) = pp.create_datasets(task, dataset, radius, device)\n",
    "print('-'*100)\n",
    "\n",
    "print('The preprocess has finished!')\n",
    "print('# of training data samples:', len(dataset_train))\n",
    "print('# of development data samples:', len(dataset_dev))\n",
    "print('# of test data samples:', len(dataset_test))\n",
    "print('-'*100)\n",
    "\n",
    "print('Creating a model.')\n",
    "model = AFA(\n",
    "        N_fingerprints, N_bond_fingerprints, dim, layer_hidden, layer_output).to(device)\n",
    "print(\" the number of fingerprints are {}\".format(N_fingerprints))\n",
    "trainer = Trainer(model)\n",
    "tester = Tester(model)\n",
    "print('# of model parameters:',\n",
    "      sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "print('-'*100)\n",
    "\n",
    "file_result = '../output/result--' + setting + '.txt'\n",
    "if task == 'classification':\n",
    "    result = 'Epoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\tAUC_test'\n",
    "if task == 'regression':\n",
    "    result = 'Epoch\\tTime(sec)\\tLoss_train\\tMAE_dev\\tMAE_test'\n",
    "\n",
    "with open(file_result, 'w') as f:\n",
    "    f.write(result + '\\n')\n",
    "\n",
    "print('Start training.')\n",
    "print('The result is saved in the output directory every epoch!')\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(iteration):\n",
    "\n",
    "    epoch += 1\n",
    "    if epoch % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss_train = trainer.train(dataset_train)\n",
    "\n",
    "    prediction_dev = tester.test(dataset_dev)\n",
    "    prediction_test = tester.test(dataset_test)\n",
    "\n",
    "    time = timeit.default_timer() - start\n",
    "\n",
    "    if epoch == 1:\n",
    "        minutes = time * iteration / 60\n",
    "        hours = int(minutes / 60)\n",
    "        minutes = int(minutes - 60 * hours)\n",
    "        print('The training will finish in about',\n",
    "              hours, 'hours', minutes, 'minutes.')\n",
    "        print('-'*100)\n",
    "        print(result)\n",
    "\n",
    "    result = '\\t'.join(map(str, [epoch, time, loss_train,\n",
    "                                 prediction_dev, prediction_test]))\n",
    "    tester.save_result(result, file_result)\n",
    "\n",
    "    print(result)\n",
    "torch.save(model, f\"{dataset}.pt\")\n",
    "atom_dict = np.load(\"atom_dict.npy\", allow_pickle=True).item()\n",
    "bond_dict = np.load(\"bond_dict.npy\", allow_pickle=True).item()\n",
    "fingerprint_dict = np.load(\"fingerprint_dict.npy\", allow_pickle=True).item()\n",
    "edge_dict = np.load(\"edge_dict.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Then we can realize the adding of properties in these following way:\n",
    "## For a given molecule, it can be formed from atoms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_dict = np.load(\"atom_dict.npy\", allow_pickle=True).item()\n",
    "bond_dict = np.load(\"bond_dict.npy\", allow_pickle=True).item()\n",
    "fingerprint_dict = np.load(\"fingerprint_dict.npy\", allow_pickle=True).item()\n",
    "bond_fingerprint_dict = np.load(\"bond_fingerprint_dict.npy\", allow_pickle=True).item()\n",
    "edge_dict = np.load(\"edge_dict.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted value for this molecule is -661.0032958984375.\n"
     ]
    }
   ],
   "source": [
    "smiles = 'CCO'\n",
    "mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "atoms = pp.create_atoms(mol, atom_dict)\n",
    "molecular_size = len(atoms)\n",
    "i_jbond_dict, bond_adjacency, bond_fingerprints, bond_index = pp.create_ijbonddict(mol, bond_dict, bond_fingerprint_dict)# for atom's bond-angle graph\n",
    "fingerprints = pp.extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                    fingerprint_dict, edge_dict)\n",
    "adjacency = Chem.GetAdjacencyMatrix(mol)# for atom's atom-bond graph\n",
    "\n",
    "fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "bond_fingerprints = torch.LongTensor(bond_fingerprints).to(device)\n",
    "adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "bond_index = torch.LongTensor(bond_index).to(device)\n",
    "property = torch.FloatTensor([[float(1.234)]]).to(device)\n",
    "info_input = (fingerprints, adjacency, bond_fingerprints, bond_adjacency, bond_index, molecular_size, property)\n",
    "result = model.forward(list(zip(*[info_input])), train=False)[0]\n",
    "fingerprints, adjacencies, bond_fingerprints, bond_adjacencies, bond_index, molecular_sizes = info_input[:-1]\n",
    "MPSs = model.embed_fingerprint(fingerprints)\n",
    "MPOs = model.embed_bond_fingerprint(bond_index)\n",
    "MPSs = model.update_MPS(adjacencies, MPSs)\n",
    "MPSs = F.normalize(MPSs, 2, 1)\n",
    "MPOs = F.normalize(MPOs, 2, 1)\n",
    "MPOs = torch.einsum(\"abc,ab->abc\", MPOs, adjacencies)\n",
    "tmp = torch.einsum(\"ac,abc->cb\", MPSs, MPOs)\n",
    "# tmp = torch.einsum(\"ab,ac->bc\", MPSs, adjacencies)\n",
    "info_sets = torch.split(tmp, molecular_sizes,dim=1)\n",
    "fv_sets = torch.split(MPSs, molecular_sizes)\n",
    "TN_results = [torch.einsum(\"bc,cb->b\", i, j) for (i,j) in zip(info_sets, fv_sets)]\n",
    "TN_results = torch.stack(TN_results)\n",
    "print(f\"the predicted value for this molecule is {result[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But also, the molecule C can be formed from fragments, like from fragment A and fragment B. Also the connection between A and B is to be calculated.\n",
    "### We first conserve the calculated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPS_partA = MPSs[:8]\n",
    "MPS_partB = MPSs[8:]\n",
    "MPO_partA = MPOs[:8,:8,:]\n",
    "MPO_partB = MPOs[8:,8:,:]\n",
    "MPO_connection = MPOs[2,8,:]\n",
    "MPS_connection1 = MPSs[2]\n",
    "MPS_connection2 = MPSs[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can calculate the property of this molecule through the adding of these two fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_partA = torch.einsum(\"ab,acb, cb->b\", MPS_partA, MPO_partA, MPS_partA)\n",
    "M_partB = torch.einsum(\"ab,acb, cb->b\", MPS_partB, MPO_partB, MPS_partB)\n",
    "M_connect = torch.einsum(\"b,b, b->b\",MPS_connection1,MPO_connection,MPS_connection2)\n",
    "TN_results = M_partA+M_partB+2*M_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It can be seen that calculate properties from fragments saves computation resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted value for this molecule is -661.0032958984375.\n"
     ]
    }
   ],
   "source": [
    "predicted_values = model.to_output(TN_results)\n",
    "predicted_values\n",
    "print(f\"the predicted value for this molecule is {predicted_values.cpu().detach().numpy()[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
